{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 14\n",
      "Memory CPU in GB: 36.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Imports\n",
    "\n",
    "from ast import arg\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import time\n",
    "\n",
    "from model import deep_GNN\n",
    "import torch\n",
    "from torch import device, nn\n",
    "import torch\n",
    "import os\n",
    "from utils.dataset import H5GeometricDataset\n",
    "from utils.eval import evaluate\n",
    "from utils.train import train_one_epoch\n",
    "from utils.configs.config import *\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from model import LitModel\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from utils.configs.config_dict import get_config\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING\n",
    "TIME_MEANS = np.load(TIME_MEANS_PATH)[0, :N_VAR]\n",
    "MEANS = np.load(GLOBAL_MEANS_PATH)[0, :N_VAR]\n",
    "STDS = np.load(GLOBAL_STDS_PATH)[0, :N_VAR]\n",
    "M = torch.as_tensor((TIME_MEANS - MEANS)/STDS)[:, 0:HEIGHT].unsqueeze(0)\n",
    "STD = torch.tensor(STDS).unsqueeze(0)\n",
    "datasets = {\n",
    "    \"train\": ConcatDataset([H5GeometricDataset(os.path.join(DATA_FILE_PATH, f\"{year}.h5\"), means=MEANS, stds=STDS) for year in YEARS]),\n",
    "    \"val\": H5GeometricDataset(VAL_FILE, means=MEANS, stds=STDS)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configs.config import BATCH_SIZE, BATCH_SIZE_VAL, LEARNING_RATE, MODEL_CONFIG\n",
    "from utils.eval import weighted_rmse_channels\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch import nn\n",
    "from torch.utils.data import DistributedSampler\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class GraphCON(nn.Module):\n",
    "    def __init__(self, GNNs, dt=1., alpha=1., gamma=1., dropout=None):\n",
    "        super(GraphCON, self).__init__()\n",
    "        self.dt = dt\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.GNNs = GNNs  # list of the individual GNN layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, X0, Y0, edge_index):\n",
    "        # set initial values of ODEs\n",
    "\n",
    "        # solve ODEs using simple IMEX scheme\n",
    "        for gnn in self.GNNs:\n",
    "            Y0 = Y0 + self.dt * (torch.relu(gnn(X0, edge_index)) -\n",
    "                                 self.alpha * Y0 - self.gamma * X0)\n",
    "            X0 = X0 + self.dt * Y0\n",
    "\n",
    "            if (self.dropout is not None):\n",
    "                Y0 = F.dropout(Y0, self.dropout, training=self.training)\n",
    "                X0 = F.dropout(X0, self.dropout, training=self.training)\n",
    "\n",
    "        return X0, Y0\n",
    "\n",
    "\n",
    "class deep_GNN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, nlayers, dt=1., alpha=1., gamma=1., dropout=None):\n",
    "        super(deep_GNN, self).__init__()\n",
    "        self.enc = nn.Linear(nfeat, nhid)\n",
    "        self.GNNs = nn.ModuleList()\n",
    "        for _ in range(nlayers):\n",
    "            self.GNNs.append(GCNConv(nhid, nhid))\n",
    "        self.graphcon = GraphCON(self.GNNs, dt, alpha, gamma, dropout)\n",
    "        self.dec = nn.Linear(nhid, nclass)\n",
    "\n",
    "    def forward(self, x0, edge_index):\n",
    "        # compute initial values of ODEs (encode input)\n",
    "        x0 = self.enc(x0)\n",
    "        # stack GNNs using GraphCON\n",
    "        x0, _ = self.graphcon(x0, x0, edge_index)\n",
    "        # decode X state of GraphCON at final time for output nodes\n",
    "        x0 = self.dec(x0)\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnn_model = deep_GNN(**MODEL_CONFIG)\n",
    "model = LitModel(datasets=datasets, std=STD, model= gnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Number of CPUs: 14\n",
      "Memory CPU in GB: 36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/Documents/eth/PMLR/venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 --- Loss: 1348.65\n",
      "Batch: 1 --- Loss: 1333.63\n",
      "Batch: 2 --- Loss: 1321.51\n",
      "Batch: 3 --- Loss: 1304.86\n",
      "Batch: 4 --- Loss: 1293.21\n",
      "Batch: 5 --- Loss: 1281.34\n",
      "Batch: 6 --- Loss: 1273.12\n",
      "Batch: 7 --- Loss: 1259.26\n",
      "Batch: 8 --- Loss: 1251.77\n",
      "Batch: 9 --- Loss: 1241.88\n",
      "Batch: 10 --- Loss: 1234.55\n",
      "Batch: 11 --- Loss: 1221.88\n",
      "Batch: 12 --- Loss: 1211.87\n",
      "Batch: 13 --- Loss: 1200.87\n",
      "Batch: 14 --- Loss: 1191.17\n",
      "Batch: 15 --- Loss: 1174.37\n",
      "Batch: 16 --- Loss: 1158.46\n",
      "Batch: 17 --- Loss: 1135.89\n",
      "Batch: 18 --- Loss: 1117.50\n",
      "Batch: 19 --- Loss: 1094.26\n",
      "Batch: 20 --- Loss: 1078.08\n",
      "Batch: 21 --- Loss: 1062.65\n",
      "Batch: 22 --- Loss: 1052.87\n",
      "Batch: 23 --- Loss: 1037.44\n",
      "Batch: 24 --- Loss: 1020.20\n",
      "Batch: 25 --- Loss: 1005.43\n",
      "Batch: 26 --- Loss: 990.45\n",
      "Batch: 27 --- Loss: 973.33\n",
      "Batch: 28 --- Loss: 953.65\n",
      "Batch: 29 --- Loss: 941.89\n",
      "Batch: 30 --- Loss: 929.26\n",
      "Batch: 31 --- Loss: 911.95\n",
      "Batch: 32 --- Loss: 892.51\n",
      "Batch: 33 --- Loss: 877.89\n",
      "Batch: 34 --- Loss: 858.37\n",
      "Batch: 35 --- Loss: 839.13\n",
      "Batch: 36 --- Loss: 820.93\n",
      "Batch: 37 --- Loss: 807.34\n",
      "Batch: 38 --- Loss: 790.81\n",
      "Epoch 1\n",
      "Batch: 0 --- Loss: 763.35\n",
      "Batch: 1 --- Loss: 750.46\n",
      "Batch: 2 --- Loss: 739.81\n",
      "Batch: 3 --- Loss: 733.40\n",
      "Batch: 4 --- Loss: 730.36\n",
      "Batch: 5 --- Loss: 725.82\n",
      "Batch: 6 --- Loss: 714.82\n",
      "Batch: 7 --- Loss: 698.69\n"
     ]
    }
   ],
   "source": [
    "dataloaders = model.train_dataloader()\n",
    "\n",
    "# %% Training\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "    for i, batch in enumerate(dataloaders):\n",
    "        loss = model.training_step(batch, 0)\n",
    "        # backwad pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Batch: {i} --- Loss: {loss.item():.2f}\")\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
