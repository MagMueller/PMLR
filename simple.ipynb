{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Imports\n",
    "\n",
    "from ast import arg\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import time\n",
    "\n",
    "from model import deep_GNN\n",
    "import torch\n",
    "from torch import device, nn\n",
    "import torch\n",
    "import os\n",
    "from utils.dataset import H5GeometricDataset\n",
    "from utils.eval import evaluate\n",
    "from utils.train import train_one_epoch\n",
    "from utils.config import *\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from model import LitModel\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING\n",
    "TIME_MEANS = np.load(TIME_MEANS_PATH)[0, :N_VAR]\n",
    "MEANS = np.load(GLOBAL_MEANS_PATH)[0, :N_VAR]\n",
    "STDS = np.load(GLOBAL_STDS_PATH)[0, :N_VAR]\n",
    "M = torch.as_tensor((TIME_MEANS - MEANS)/STDS)[:, 0:HEIGHT].unsqueeze(0)\n",
    "STD = torch.tensor(STDS).unsqueeze(0)\n",
    "datasets = {\n",
    "    \"train\": ConcatDataset([H5GeometricDataset(os.path.join(DATA_FILE_PATH, f\"{year}.h5\"), means=MEANS, stds=STDS) for year in YEARS]),\n",
    "    \"val\": H5GeometricDataset(VAL_FILE, means=MEANS, stds=STDS)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import BATCH_SIZE, BATCH_SIZE_VAL, LEARNING_RATE, MODEL_CONFIG\n",
    "from utils.eval import weighted_rmse_channels\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch import nn\n",
    "from torch.utils.data import DistributedSampler\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class GraphCON(nn.Module):\n",
    "    def __init__(self, GNNs, dt=1., alpha=1., gamma=1., dropout=None):\n",
    "        super(GraphCON, self).__init__()\n",
    "        self.dt = dt\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.GNNs = GNNs  # list of the individual GNN layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, X0, Y0, edge_index):\n",
    "        # set initial values of ODEs\n",
    "\n",
    "        # solve ODEs using simple IMEX scheme\n",
    "        for gnn in self.GNNs:\n",
    "            Y0 = Y0 + self.dt * (torch.relu(gnn(X0, edge_index)) -\n",
    "                                 self.alpha * Y0 - self.gamma * X0)\n",
    "            X0 = X0 + self.dt * Y0\n",
    "\n",
    "            if (self.dropout is not None):\n",
    "                Y0 = F.dropout(Y0, self.dropout, training=self.training)\n",
    "                X0 = F.dropout(X0, self.dropout, training=self.training)\n",
    "\n",
    "        return X0, Y0\n",
    "\n",
    "\n",
    "class deep_GNN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, nlayers, dt=1., alpha=1., gamma=1., dropout=None):\n",
    "        super(deep_GNN, self).__init__()\n",
    "        self.enc = nn.Linear(nfeat, nhid)\n",
    "        self.GNNs = nn.ModuleList()\n",
    "        for _ in range(nlayers):\n",
    "            self.GNNs.append(GCNConv(nhid, nhid))\n",
    "        self.graphcon = GraphCON(self.GNNs, dt, alpha, gamma, dropout)\n",
    "        self.dec = nn.Linear(nhid, nclass)\n",
    "\n",
    "    def forward(self, x0, edge_index):\n",
    "        # compute initial values of ODEs (encode input)\n",
    "        x0 = self.enc(x0)\n",
    "        # stack GNNs using GraphCON\n",
    "        x0, _ = self.graphcon(x0, x0, edge_index)\n",
    "        # decode X state of GraphCON at final time for output nodes\n",
    "        x0 = self.dec(x0)\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnn_model = deep_GNN(**MODEL_CONFIG)\n",
    "model = LitModel(datasets=datasets, std=STD, model= gnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 14\n",
      "Memory CPU in GB: 36.0\n",
      "Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus/Documents/eth/PMLR/venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1328.1466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 1\n",
      "tensor(1329.7198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 2\n",
      "tensor(1330.9776, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 3\n",
      "tensor(1327.4208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 4\n",
      "tensor(1327.8705, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 5\n",
      "tensor(1328.2026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 6\n",
      "tensor(1329.8271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 7\n",
      "tensor(1327.5175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 8\n",
      "tensor(1332.5893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 9\n",
      "tensor(1336.6306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 10\n",
      "tensor(1341.4285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 11\n",
      "tensor(1343.7970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 12\n",
      "tensor(1350.7523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 13\n",
      "tensor(1358.5043, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 14\n",
      "tensor(1365.0136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 15\n",
      "tensor(1365.9623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 16\n",
      "tensor(1369.6405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 17\n",
      "tensor(1368.4129, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 18\n",
      "tensor(1366.8564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Batch 19\n"
     ]
    }
   ],
   "source": [
    "dataloaders = model.train_dataloader()\n",
    "\n",
    "# %% Training\n",
    "\n",
    "for i, batch in enumerate(dataloaders):\n",
    "    # sample \n",
    "    print(f\"Batch {i}\")\n",
    "\n",
    "    loss = model.training_step(batch, 0)\n",
    "    print(f\"Batch: {i} --- Loss: {loss.item():.2f}\")\n",
    "    # backwad pass\n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
